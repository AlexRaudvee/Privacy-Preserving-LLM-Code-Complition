{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a792f7c",
   "metadata": {},
   "source": [
    "# Privacy–Utility Trade-off for Code Completion (HumanEval)\n",
    "\n",
    "This notebook:\n",
    "1. Loads the first 20 examples from `openai/openai_humaneval` (test split).\n",
    "2. Builds three prompt variants per example:\n",
    "   - Original\n",
    "   - Low obfuscation (variable renaming)\n",
    "   - High obfuscation (strip comments/docstrings + placeholders)\n",
    "3. Generates one completion per prompt using `Salesforce/codet5-small` (swap model if desired).\n",
    "4. Computes:\n",
    "   - **Utility score**: ROUGE-L (F1) between generated completion and `canonical_solution`\n",
    "   - **Privacy score**: normalized Levenshtein distance between obfuscated prompt and original prompt\n",
    "5. Plots Privacy vs Utility.\n",
    "\n",
    "Notes:\n",
    "- Utility metric here is intentionally simple; it does not verify correctness.\n",
    "- Privacy metric here is a proxy for *prompt similarity* (not a formal privacy guarantee).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5fd8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab:\n",
    "# !pip install -r ../requirements.txt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "from src.utils import low_obfuscation, high_obfuscation, privacy_score, utility_score_rougeL\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load dataset (first 20 examples from test split)\n",
    "\n",
    "ds = load_dataset(\"openai/openai_humaneval\", split=\"test\")\n",
    "ds20 = ds.select(range(20))\n",
    "\n",
    "# Quick peek\n",
    "ds20[0].keys(), ds20[0][\"task_id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a55ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Prepare prompt variants\n",
    "\n",
    "rows = []\n",
    "for ex in ds20:\n",
    "    original = ex[\"prompt\"]\n",
    "    low = low_obfuscation(original)\n",
    "    high = high_obfuscation(original)\n",
    "    rows.append({\n",
    "        \"task_id\": ex[\"task_id\"],\n",
    "        \"prompt_original\": original,\n",
    "        \"prompt_low\": low,\n",
    "        \"prompt_high\": high,\n",
    "        \"canonical_solution\": ex[\"canonical_solution\"],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Load model (default: CodeT5-small)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = os.environ.get(\"MODEL_NAME\", \"Salesforce/codet5-small\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "device, model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(prompt: str,\n",
    "                        max_new_tokens: int = 128,\n",
    "                        temperature: float = 0.8,\n",
    "                        top_p: float = 0.95) -> str:\n",
    "    \"\"\"Single sampled completion.\"\"\"\n",
    "    inputs = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    text = tok.decode(out[0], skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "# Sanity check (first example, original prompt)\n",
    "print(df.loc[0, \"task_id\"])\n",
    "print(generate_completion(df.loc[0, \"prompt_original\"])[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a4e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Generate 60 completions total (20 × 3)\n",
    "\n",
    "outputs = []\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    for condition, prompt_col in [(\"original\", \"prompt_original\"),\n",
    "                                  (\"low\", \"prompt_low\"),\n",
    "                                  (\"high\", \"prompt_high\")]:\n",
    "        prompt = row[prompt_col]\n",
    "        comp = generate_completion(prompt)\n",
    "\n",
    "        outputs.append({\n",
    "            \"task_id\": row[\"task_id\"],\n",
    "            \"condition\": condition,\n",
    "            \"prompt_used\": prompt,\n",
    "            \"prompt_original\": row[\"prompt_original\"],\n",
    "            \"canonical_solution\": row[\"canonical_solution\"],\n",
    "            \"completion\": comp,\n",
    "        })\n",
    "\n",
    "out_df = pd.DataFrame(outputs)\n",
    "out_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c0326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Score utility + privacy\n",
    "\n",
    "out_df[\"privacy\"] = out_df.apply(lambda r: privacy_score(r[\"prompt_used\"], r[\"prompt_original\"]), axis=1)\n",
    "out_df[\"utility\"] = out_df.apply(lambda r: utility_score_rougeL(r[\"completion\"], r[\"canonical_solution\"]), axis=1)\n",
    "\n",
    "out_df.groupby(\"condition\")[[\"privacy\",\"utility\"]].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8830dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Scatter plot: Privacy vs Utility\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "for cond in [\"original\", \"low\", \"high\"]:\n",
    "    sub = out_df[out_df[\"condition\"] == cond]\n",
    "    plt.scatter(sub[\"privacy\"], sub[\"utility\"], label=cond, alpha=0.8)\n",
    "\n",
    "plt.xlabel(\"Privacy Score (normalized Levenshtein distance)\")\n",
    "plt.ylabel(\"Utility Score (ROUGE-L F1 vs canonical_solution)\")\n",
    "plt.title(\"Privacy–Utility Trade-off (HumanEval, n=20, 1 completion/condition)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f63064c",
   "metadata": {},
   "source": [
    "## Brief analysis (draft)\n",
    "\n",
    "Fill in after you run:\n",
    "\n",
    "- **Expected trend**: as obfuscation increases, prompts diverge more from the original (privacy proxy ↑), but the model may lose signal and produce less overlapping text with the canonical solution (utility ↓).\n",
    "- **Common failure mode**: lexical metrics (ROUGE) can drop even when the completion is functionally correct (e.g., different variable names or alternative implementations).\n",
    "- **Interpretation caution**: normalized edit distance is not a formal privacy guarantee; it measures only text-level dissimilarity, not whether sensitive semantics can still be inferred.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
